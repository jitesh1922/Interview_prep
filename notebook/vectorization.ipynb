{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocabulary: ['and' 'dewangan' 'document' 'first' 'is' 'jitesh' 'one' 'second' 'the'\n",
      " 'third' 'this']\n",
      "Document-Term Matrix:\n",
      "[[0 0 1 1 1 1 0 0 1 0 1]\n",
      " [0 1 2 0 1 0 0 1 1 0 1]\n",
      " [1 0 0 0 1 0 1 0 1 1 1]\n",
      " [0 0 1 1 1 0 0 0 1 0 1]]\n"
     ]
    }
   ],
   "source": [
    "# Example text corpus\n",
    "\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "corpus = [\n",
    "    \"This is the first document. jitesh \",\n",
    "    \"This document is the second document.  dewangan\",\n",
    "    \"And this is the third one.\",\n",
    "    \"Is this the first document?\",\n",
    "]\n",
    "\n",
    "# Create an instance of the CountVectorizer\n",
    "vectorizer = CountVectorizer()\n",
    "# Fit and transform the corpus into a document-term matrix\n",
    "X = vectorizer.fit_transform(corpus)\n",
    "\n",
    "# Print the vocabulary and the document-term matrix\n",
    "print(\"Vocabulary:\", vectorizer.get_feature_names_out())\n",
    "print(\"Document-Term Matrix:\")\n",
    "print(X.toarray())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "vocab:  ['and' 'and this' 'and this is' 'dewangan' 'document' 'document dewangan'\n",
      " 'document is' 'document is the' 'document jitesh' 'first'\n",
      " 'first document' 'first document jitesh' 'is' 'is the' 'is the first'\n",
      " 'is the second' 'is the third' 'is this' 'is this the' 'jitesh' 'one'\n",
      " 'second' 'second document' 'second document dewangan' 'the' 'the first'\n",
      " 'the first document' 'the second' 'the second document' 'the third'\n",
      " 'the third one' 'third' 'third one' 'this' 'this document'\n",
      " 'this document is' 'this is' 'this is the' 'this the' 'this the first']\n",
      "Document-Term Matrix:\n",
      "[[0 0 0 0 1 0 0 0 1 1 1 1 1 1 1 0 0 0 0 1 0 0 0 0 1 1 1 0 0 0 0 0 0 1 0 0\n",
      "  1 1 0 0]\n",
      " [0 0 0 1 2 1 1 1 0 0 0 0 1 1 0 1 0 0 0 0 0 1 1 1 1 0 0 1 1 0 0 0 0 1 1 1\n",
      "  0 0 0 0]\n",
      " [1 1 1 0 0 0 0 0 0 0 0 0 1 1 0 0 1 0 0 0 1 0 0 0 1 0 0 0 0 1 1 1 1 1 0 0\n",
      "  1 1 0 0]\n",
      " [0 0 0 0 1 0 0 0 0 1 1 0 1 0 0 0 0 1 1 0 0 0 0 0 1 1 1 0 0 0 0 0 0 1 0 0\n",
      "  0 0 1 1]]\n"
     ]
    }
   ],
   "source": [
    "## unigram and bigram \n",
    "## ngram_range=(min, max) -> use all n-gram in this range\n",
    "vectorizer2 = CountVectorizer(analyzer='word', ngram_range=(1, 3))\n",
    "X2 = vectorizer2.fit_transform(corpus)\n",
    "print(\"vocab: \" , vectorizer2.get_feature_names_out())\n",
    "print(\"Document-Term Matrix:\")\n",
    "print(X2.toarray())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "vocab:  ['and' 'dewangan' 'document' 'first' 'is' 'jitesh' 'one' 'second' 'the'\n",
      " 'third' 'this']\n",
      "Document-Term Matrix:\n",
      "[[0.         0.         0.37835697 0.46734613 0.30933162 0.59276931\n",
      "  0.         0.         0.30933162 0.         0.30933162]\n",
      " [0.         0.47422682 0.60538568 0.         0.24747123 0.\n",
      "  0.         0.47422682 0.24747123 0.         0.24747123]\n",
      " [0.51184851 0.         0.         0.         0.26710379 0.\n",
      "  0.51184851 0.         0.26710379 0.51184851 0.26710379]\n",
      " [0.         0.         0.46979139 0.58028582 0.38408524 0.\n",
      "  0.         0.         0.38408524 0.         0.38408524]]\n"
     ]
    }
   ],
   "source": [
    "#using TFIDF\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "corpus = [\n",
    "    \"This is the first document. jitesh \",\n",
    "    \"This document is the second document.  dewangan\",\n",
    "    \"And this is the third one.\",\n",
    "    \"Is this the first document?\",\n",
    "]\n",
    "\n",
    "vectorizer3 = TfidfVectorizer()\n",
    "X3 = vectorizer3.fit_transform(corpus)\n",
    "print(\"vocab: \" , vectorizer3.get_feature_names_out())\n",
    "print(\"Document-Term Matrix:\")\n",
    "print(X3.toarray())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "## using word2vec\n",
    "from gensim.models import Word2Vec\n",
    "def corpus_to_token(corpus):\n",
    "    token = []\n",
    "    for sent in corpus:\n",
    "        #print(sent)\n",
    "        token.append(sent.lower().split())\n",
    "        #print(token)\n",
    "    return token\n",
    "\n",
    "\n",
    "tokenizedtext  = corpus_to_token(corpus)\n",
    "word2vec_model = Word2Vec(sentences=tokenizedtext, vector_size=100, min_count=1, workers=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['This', 'first', 'document.', 'jitesh']]\n",
      "[['This', 'first', 'document.', 'jitesh'], ['This', 'document', 'second', 'document.', 'dewangan']]\n",
      "[['This', 'first', 'document.', 'jitesh'], ['This', 'document', 'second', 'document.', 'dewangan'], ['And', 'third', 'one.']]\n",
      "[['This', 'first', 'document.', 'jitesh'], ['This', 'document', 'second', 'document.', 'dewangan'], ['And', 'third', 'one.'], ['Is', 'first', 'document?']]\n",
      "[['This', 'first', 'document.', 'jitesh'], ['This', 'document', 'second', 'document.', 'dewangan'], ['And', 'third', 'one.'], ['Is', 'first', 'document?']]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /Users/jiteshdewangan/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "# STOP OWRLD removal from tkt\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "nltk.download('stopwords')\n",
    "stop_words = set(stopwords.words('english'))\n",
    "def remove_stopwords(tokens):\n",
    "    token = []\n",
    "    for sent in corpus:\n",
    "        vocab = sent.split()\n",
    "        token.append([word for word in vocab if word not in stop_words])\n",
    "        print(token)\n",
    "    return token\n",
    "    #return [word for word in tokens if word not in stop_words]\n",
    "print(remove_stopwords(corpus))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "3.10.11",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
